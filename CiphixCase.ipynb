{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P_zh2woUdK_m"
      },
      "source": [
        "# Ciphix Machine Learning Case\n",
        "Door: Sander Boers\n",
        "\n",
        "\n",
        "Dit is een opdracht dat onderdeel is van de sollicatie procedure voor Ciphix. Het doel is om uit groot tekstbestand machine-learning technieken toe te passen, om 'topics' te bepalen typerend voor een gesprek."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset\n",
        "There is an option to shrink the dataset to have shorter runtime. Add the following code to end of `pd.read_csv`:\n",
        "1. `.head(n=10000)` for the first n rows, or\n",
        "2. `.sample()` if you want a random sample.\n",
        "\n",
        "Also there is the option to only fit to the customer data (`ONLY_CUSTOMERS = True`) and not the replies of particular companies. It checks if a datarow has an `@123456` (or any other numeric combination) inside, this implies a response to a customer and often does not give additional information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s3xyhRlNdHZQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Dataset will be downloaded if it is not already there\n",
        "if os.path.exists('data.csv'):\n",
        "    data = pd.read_csv('data.csv', names=['text']).head(n=10000)\n",
        "else:\n",
        "    data = pd.read_csv('https://ciphix.io/ai/data.csv', names=['text'])\n",
        "\n",
        "\n",
        "ONLY_CUSTOMERS = True\n",
        "if ONLY_CUSTOMERS:\n",
        "    filter = data['text'].str.contains(r'@([0-9]+)')\n",
        "    data = data[~filter]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing data\n",
        "This is an important step in Topic modeling. It consists out of multiple processes, the first is removing information we do not need. This includes:\n",
        "1. URLs, \n",
        "2. mentions (words starting with '@'), \n",
        "3. the customer service employee signature (e.g. ^JK),\n",
        "4. any symbols, emojis or non-western charachters\n",
        "\n",
        "Next, all words are put into lowercase and will be tokenized, meaning that all sentences will be split into words.\n",
        "\n",
        "Then, there are three remaining steps:\n",
        "We look for bigrams and trigrams, these are words that frequently occur together and will be appended to one word (e.g. 'customer', 'service' will become 'customer_service').\n",
        "Also stopwords will be removed, stopwords do not contribute much to the meaning of a sentence. Stopwords are for example: 'only', 'would', 'some', 'everyone'.\n",
        "\n",
        "And finally words will be lemmetizized. This step will convert words to its base form, so for example: 'dogs' becomes 'dog' and 'tried' becomes 'try'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "yoR7uyXfflCc",
        "outputId": "bcbb0e40-3521-4c2a-a813-c7804e011c0b"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "import gensim\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable = [\"parser\", \"ner\"])\n",
        "allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
        "\n",
        "def preprocess(doc):\n",
        "  # Regex to remove URLs\n",
        "  doc = re.sub(r'\\bhttps?://\\S+\\b', '', doc) \n",
        "\n",
        "  # Regex to remove mentions, e.g. @UberSupport\n",
        "  # doc = re.sub(r'@\\w+\\b', '', doc) \n",
        "\n",
        "  # Regex to remove signatures starting with ('^', '-' or '*')\n",
        "  doc = re.sub(r'\\B[-^*&]\\s*\\w+', '', doc) \n",
        "  \n",
        "  # Regex to remove any symbols, emojis or non-western charachters\n",
        "  doc = re.sub(r'[^a-zA-Z0-9\\s,.?!;:()]+', '', doc) \n",
        "  \n",
        "  # Function to convert document into lowercase, de-accents and tokenize\n",
        "  doc = gensim.utils.simple_preprocess(doc, deacc=True, min_len=2)\n",
        "\n",
        "  return doc\n",
        "  \n",
        "\n",
        "data['preprocessed_text'] = data['text'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'together', 'it', 'seeming', 'n’t', 'made', 'really', \"'re\", '’d', 'except', 'out', 'mostly', '’re', 'would', 'wherever', 'already', 'though', 'how', 'those', 'own', 'be', 'onto', 'she', 're', 'say', 'something', 'empty', \"'m\", 'ourselves', 'never', 'hereafter', 'at', 'beforehand', '‘s', '’m', 'are', 'most', 'rather', 'amongst', 'they', 'hundred', 'unless', 'until', 'its', 'make', 'third', 'perhaps', 'more', 'else', 'hence', 'across', 'their', 'wherein', 'very', 'sixty', 'therein', 'mine', '‘m', 'his', 'so', 'my', 'back', 'as', 'although', 'might', 'hi', 'hello', 'myself', 'sometimes', 'fifty', 'forty', 'may', 'yours', 'on', 'eleven', 'next', 'full', 'off', 'when', 'three', 'than', 'bottom', 'quite', 'whereafter', 'thus', 'further', 'nothing', 'almost', 'through', 'who', 'whole', 'if', 'is', 'take', 'whose', 'throughout', 'beyond', 'either', 'call', 'anywhere', 'few', 'others', 'everyone', 'over', 'toward', 'below', 'because', 'we', 'eight', 'yourself', 'from', 'often', \"'d\", 'ill', 'yourselves', 'him', 'n‘t', 'himself', 'not', 'whoever', 'doing', 'me', 'please', 'moreover', 'otherwise', 'only', 'two', 'up', '’ll', 'every', 'why', '‘ll', 'keep', 'then', 'everything', 'even', 'them', 'one', 'neither', 'had', 'five', 'towards', 'many', 'around', 'hereby', 'six', 'under', 'by', 'beside', 'well', 'about', 'the', 'between', 'no', 'ours', 'same', 'former', 'move', 'ever', 'always', 'several', 'her', 'each', 'thence', 'a', 'what', 'whereupon', 'above', 'in', \"n't\", 'four', 'where', 'least', '‘ve', 'using', 'show', 'do', 'someone', 'yet', 'becomes', 'been', 'less', 'just', 'while', 'whether', 'somehow', 'top', 'during', 'all', 'of', 'this', 'regarding', 'ca', '‘re', 'since', 'twelve', 'could', 'us', 'latter', 'also', 'none', 'behind', 'did', 'put', 'an', 'which', 'noone', 'however', 'once', 'name', 'became', 'ten', 'et', 'besides', 'both', 'become', 'see', 'too', 'another', 'front', 'these', 'due', 'such', 'nowhere', 'first', 'he', 'anyhow', 'done', 'any', 'again', 'meanwhile', 'upon', 'down', 'whither', 'amount', 'alone', 'go', 'nevertheless', 'whence', '‘d', 'elsewhere', 'your', 'whatever', 'can', 'and', 'somewhere', 'into', 'hey', 'thereupon', \"'ve\", 'whenever', 'itself', '’s', 'nobody', 'serious', 'hers', 'i', 'fifteen', 'will', 'nor', 'thereby', 'anything', 'for', 'side', 'were', 'has', 'being', 'enough', 'other', 'herein', 'have', 'seem', 'anyone', 'whom', 'along', 'used', 'here', 'formerly', 'thereafter', '’ve', 'our', 'twenty', 'some', 'still', 'latterly', 'everywhere', 'via', 'among', 'part', 'there', 'seemed', 'against', 'sometime', 'themselves', 'herself', 'but', 'before', 'whereby', 'was', 'get', 'afterwards', 'therefore', 'cannot', 'indeed', 'seems', 'give', 'now', 'without', 'within', 'or', 'hereupon', 'you', 'becoming', 'am', 'various', 'nine', 'does', 'must', 'thru', 'after', 'namely', 'last', \"'ll\", 'that', 'much', 'to', 'should', \"'s\", 'per', 'whereas', 'anyway', 'with'}\n"
          ]
        }
      ],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data['preprocessed_text'], min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data['preprocessed_text']], threshold=100)  \n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "\n",
        "stopwords = nlp.Defaults.stop_words\n",
        "stopwords.update([\"hi\", \"hello\", \"hey\", \"et\", \"ill\"])\n",
        "\n",
        "def apply_trigrams(doc):\n",
        "  doc = [token for token in doc if token not in stopwords]\n",
        "  doc = trigram_mod[bigram_mod[doc]]\n",
        "  return doc\n",
        "\n",
        "data['trigram_text'] = data['preprocessed_text'].apply(apply_trigrams)\n",
        "data = data[data['trigram_text'].apply(len) > 2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_lemmatization(doc):\n",
        "  doc = nlp(\" \".join(doc))\n",
        "  doc = [token.lemma_ for token in doc if token.pos_ in allowed_postags and len(token) > 2]\n",
        "  return doc\n",
        "\n",
        "data['lemmatization_text'] = data['trigram_text'].apply(apply_lemmatization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "dictionary = Dictionary(data['lemmatization_text'])\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data['lemmatization_text']]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the dataset to LDA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "lda_model = LdaModel(corpus = corpus, \n",
        "                     id2word = dictionary, \n",
        "                     num_topics = 10, \n",
        "                     passes = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0: 0.027*\"month\" + 0.026*\"issue\" + 0.025*\"internet\" + 0.022*\"comcastcare\" + 0.021*\"work\" + 0.016*\"pay\" + 0.014*\"time\" + 0.013*\"idea\" + 0.013*\"ideacare\" + 0.013*\"talk\"\n",
            "Topic 1: 0.043*\"southwestair\" + 0.038*\"good\" + 0.032*\"thank\" + 0.032*\"chipotletweet\" + 0.022*\"year\" + 0.016*\"get\" + 0.016*\"great\" + 0.015*\"love\" + 0.010*\"food\" + 0.009*\"delta\"\n",
            "Topic 2: 0.096*\"service\" + 0.071*\"customer\" + 0.030*\"upshelp\" + 0.022*\"call\" + 0.020*\"bad\" + 0.016*\"phone\" + 0.015*\"help\" + 0.015*\"answer\" + 0.014*\"day\" + 0.014*\"care\"\n",
            "Topic 3: 0.053*\"help\" + 0.037*\"email\" + 0.036*\"need\" + 0.035*\"account\" + 0.034*\"number\" + 0.025*\"check\" + 0.025*\"book\" + 0.023*\"charge\" + 0.023*\"driver\" + 0.021*\"contact\"\n",
            "Topic 4: 0.045*\"day\" + 0.031*\"train\" + 0.027*\"week\" + 0.026*\"time\" + 0.023*\"virgintrain\" + 0.019*\"today\" + 0.018*\"gwrhelp\" + 0.018*\"swhelp\" + 0.017*\"late\" + 0.016*\"go\"\n",
            "Topic 5: 0.083*\"flight\" + 0.051*\"americanair\" + 0.033*\"britishairway\" + 0.026*\"ticket\" + 0.021*\"seat\" + 0.020*\"hour\" + 0.018*\"delay\" + 0.016*\"fly\" + 0.016*\"travel\" + 0.014*\"min\"\n",
            "Topic 6: 0.040*\"think\" + 0.032*\"wait\" + 0.032*\"long\" + 0.026*\"right\" + 0.023*\"minute\" + 0.021*\"time\" + 0.020*\"point\" + 0.019*\"price\" + 0.016*\"take\" + 0.015*\"return\"\n",
            "Topic 7: 0.060*\"order\" + 0.030*\"buy\" + 0.027*\"say\" + 0.027*\"store\" + 0.026*\"amazonhelp\" + 0.026*\"deliver\" + 0.023*\"item\" + 0.022*\"delivery\" + 0.018*\"package\" + 0.017*\"game\"\n",
            "Topic 8: 0.128*\"thank\" + 0.074*\"send\" + 0.029*\"reply\" + 0.028*\"money\" + 0.028*\"response\" + 0.021*\"message\" + 0.017*\"email\" + 0.016*\"link\" + 0.015*\"detail\" + 0.014*\"know\"\n",
            "Topic 9: 0.044*\"applesupport\" + 0.039*\"try\" + 0.034*\"work\" + 0.030*\"update\" + 0.027*\"spotifycare\" + 0.026*\"phone\" + 0.026*\"app\" + 0.018*\"help\" + 0.017*\"new\" + 0.017*\"fix\"\n"
          ]
        }
      ],
      "source": [
        "for topic_id in range(10):\n",
        "    print(f'Topic {topic_id}: {lda_model.print_topic(topic_id)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyLDAvis'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyLDAvis\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyLDAvis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgensim_models\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
          ]
        }
      ],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Visualize with pyLDAvis: See [2] for more details\n",
        "pyLDAvis.enable_notebook()\n",
        "visualization = pyLDAvis.gensim_models.prepare(\n",
        "    lda_model, \n",
        "    corpus,\n",
        "    dictionary, \n",
        "    mds = \"mmds\", \n",
        "    R = 30)\n",
        "\n",
        "visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wordcloud'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 1. Wordcloud of Top N words in each topic\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwordcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m WordCloud\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(lda_model\u001b[39m.\u001b[39mnum_topics):\n\u001b[0;32m      6\u001b[0m     plt\u001b[39m.\u001b[39mfigure()\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
          ]
        }
      ],
      "source": [
        "# 1. Wordcloud of Top N words in each topic\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "for t in range(lda_model.num_topics):\n",
        "    plt.figure()\n",
        "    plt.gca().set_title('Topic ' + str(t))\n",
        "    plt.imshow(WordCloud(background_color=\"rgba(255, 255, 255, 0)\", mode=\"RGBA\", width=1600, height=800).fit_words(dict(lda_model.show_topic(t, 50))))\n",
        "    plt.axis(\"off\")\n",
        "    plt.gcf().savefig(f\"./static/wordclouds/topic{t}.png\", transparent=True, dpi=300)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'lda_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Save models\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m lda_model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39m./saved_models/LDAmodel\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m bigram_mod\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39m./saved_models/bigram_mod.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m trigram_mod\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39m./saved_models/trigram_mod.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'lda_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Save models\n",
        "\n",
        "lda_model.save('./saved_models/LDAmodel')\n",
        "bigram_mod.save(\"./saved_models/bigram_mod.pkl\")\n",
        "trigram_mod.save(\"./saved_models/trigram_mod.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "f1113036c183090e15e899b5aa6b1a1578a0f830d371bb1d9007920ba5ecfae1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
